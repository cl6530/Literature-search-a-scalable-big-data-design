{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05832c3-ffeb-414d-96e8-1ad0c9a2d422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/chenxiliu/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.3.23-cp38-cp38-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1 regex-2023.3.23\n"
     ]
    }
   ],
   "source": [
    "#pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed540d3-2c2f-4082-bd53-4fe4f7d08884",
   "metadata": {},
   "source": [
    " The fist part scrapes all the contents from each url within the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec015f78-f6b0-48ba-9433-db6e9145d515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf90ad7-25bb-43ab-a0cc-ba9bf5ab8ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is a function to get all the links that contains the content of the txt books.\n",
    "The max depth of link within link is 2, to retrieve all links, we check all link start with 'http://www.authorama.com/'\n",
    "'''\n",
    "def get_all_links(url, depth=0, max_depth=1):\n",
    "    if depth > max_depth:\n",
    "        return []\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        absolute_url = urljoin(url, href)\n",
    "        if absolute_url.startswith('http://www.authorama.com/'):\n",
    "            links.append(absolute_url)\n",
    "            nested_links = get_all_links(absolute_url, depth=depth+1, max_depth=max_depth)\n",
    "            links.extend(nested_links)\n",
    "\n",
    "    return links\n",
    "\n",
    "url = 'http://www.authorama.com/'\n",
    "book_links = get_all_links(url)\n",
    "\n",
    "#print(book_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b6dfdc4-5207-43a2-afc4-ee8fbaf4c798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_book_text(book_url, output_dir):\n",
    "    response = requests.get(book_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Create a unique filename for each book chapter based on its URL\n",
    "    filename = os.path.join(output_dir, book_url.split('/')[-1] + '.txt')\n",
    "\n",
    "    # Save the content to the file directly, without storing it in memory\n",
    "    with open(filename, 'w') as file:\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            file.write(paragraph.text + '\\n')\n",
    "\n",
    "\n",
    "output_dir = 'your local path'\n",
    "for book_link in book_links:\n",
    "    get_book_text(book_link, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109214f7-ae87-48b2-9055-7dcf37cb8c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
